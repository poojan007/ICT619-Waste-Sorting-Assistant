{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw/NGaXYsigM5CyGyuvm/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojan007/ICT619-Waste-Sorting-Assistant/blob/main/ICT619-Assignment-1-Data-Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SmartSort Model\n",
        "This model provides outlines a process for loading augmenting, and training a convolutional neural network (CNN) on image data for a binary classification task.\n",
        "\n",
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "b0ShL6qcvVyq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9tM2Dzlm_PO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define dataset paths"
      ],
      "metadata": {
        "id": "jpts8eGZwFpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/data/DATASET/TRAIN'\n",
        "validation_dir = '/content/data/DATASET/TEST'"
      ],
      "metadata": {
        "id": "XMkE9NhgwIyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "A data augmentation pipeline is created using a 'Sequential' model, which includes random flips, rotations, zooms, and translations. This is to artifically expland the training dataset by generating modifiied versions of the training images, helping the model generalize better."
      ],
      "metadata": {
        "id": "6hAu5T0O14wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "  layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "  layers.experimental.preprocessing.RandomTranslation(height_factor=0.2, width_factor=0.2)\n",
        "])\n"
      ],
      "metadata": {
        "id": "GFWIa-xe2OUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the datasets\n",
        "\n",
        "The training and validation datasets are loaded from directories, specifying image size, batch size, and how the data is split. The 'validation_split' parameter"
      ],
      "metadata": {
        "id": "JzbNRgaF2XyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "CQ5Xypwg2vnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring Datasets for Performance\n",
        "\n",
        "These lines configure the datasets for performance '.cache()' keeps the images in memory after they're loaded off disk during the first epoch, '.shuffle()' randamizes the order of the images to reduce model overfitting, and '.prefetch()' overlaps data preprocessing and model execution while training."
      ],
      "metadata": {
        "id": "aCIdLLMr20IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "hWy-njOM3Kas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building\n",
        "\n",
        "Building the CNN model architecture. It starts with the data augmentation layer, followed by a rescaling layer to normalize pixel values. Then, it adds convolutional and max pooling layers for feature extraction, followed by dense layers for classification. The final layer uses a sigmoid activation function suitable for binary classification."
      ],
      "metadata": {
        "id": "Y9uKJDJ23Nt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    data_augmentation,\n",
        "    layers.experimental.preprocessing.Rescaling(1./255),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "cNYPKyoc3ltm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile the Model\n",
        "\n",
        "The model is compiled with the Adam optimizer and binary crossentropy loss function, which is appropriate for binary classification tasks. The model's performance will be evaluated based on accuracy."
      ],
      "metadata": {
        "id": "C_PhT4yh3n5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fp2XR0XP35ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Finally, the model is trained for a defined number of epochs using the training dataset, with validation performed on the validation dataset. The 'fit' method returns a history object containing training and validation loss and accuracy for each epoch, which can be used for analysis of the model's performance over time."
      ],
      "metadata": {
        "id": "yFVQGOmn36tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=10\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")\n"
      ],
      "metadata": {
        "id": "dO0k4CAy4QPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validating the model"
      ],
      "metadata": {
        "id": "Io40dKa14hp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prepare and predict the class of an image\n",
        "def predict_with_augmentation(img_path, model):\n",
        "    # Load the image\n",
        "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "\n",
        "    # Convert the image to a numpy array\n",
        "    img_array = image.img_to_array(img)\n",
        "\n",
        "    # Expand dimensions to match the shape of model input\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Predict (data augmentation is applied automatically)\n",
        "    prediction = model.predict(img_array)\n",
        "\n",
        "    # Assuming binary classification with a sigmoid activation, adjust if necessary\n",
        "    predicted_class = 'Class 1' if prediction[0] > 0.5 else 'Class 0'\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "# Example usage\n",
        "img_path = 'path/to/your/image.jpg'\n",
        "predicted_class = predict_with_augmentation(img_path, model)\n",
        "print(f'Predicted class: {predicted_class}')"
      ],
      "metadata": {
        "id": "vyb7Ycvq4kwV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}